{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of LSTMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, GRU\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import regularizers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_percentage = 0.9\n",
    "explained_variance = 0.9\n",
    "df = pd.read_csv(\"data/rescuetime_data-ac-min.csv\")\n",
    "data_pd = Clean_DF(df)\n",
    "data_pd.clean_data(time_percentage=time_percentage)\n",
    "data_pd.clean_df = data_pd.clean_df.reset_index()\n",
    "data_pd.get_pca(explained_variance=explained_variance)\n",
    "data_pd.get_day_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "# with open('data_pd_80.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump(data_pd, f)\n",
    "\n",
    "# # Getting back the objects:\n",
    "with open('data_pd.pickle', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    data_pd = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (16704, 9) \n",
      "\n",
      "Number of apps that consume 90.0 % of all users time:  99 \n",
      "\n",
      "Cleaned dataset columns: \n",
      " ['Date' 'Time Spent (seconds)' 'Activity' 'Category' 'Productivity'\n",
      " 'Activity Vector' 'Productivity Score' 'Day' 'Time'] \n",
      "\n",
      "Number of components that explain 90.0 % of the data:  30 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", data_pd.clean_df.shape,'\\n')\n",
    "print(\"Number of apps that consume\", time_percentage*100, \"% of all users time: \",len(data_pd.popular_apps), '\\n')\n",
    "print(\"Cleaned dataset columns:\",'\\n', data_pd.clean_df.columns.values, '\\n')\n",
    "print(\"Number of components that explain\", explained_variance*100,\"% of the data: \",data_pd.pca_data.shape[1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.flags\n",
    "FLAGS.look_back = 24\n",
    "FLAGS.batch_size = 8\n",
    "FLAGS.inputlength = data_pd.activity_vector.shape[1]\n",
    "np.random.seed(7)\n",
    "dataset = data_pd.activity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13363 3341\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = create_dataset(train, FLAGS.look_back)\n",
    "testX, testY = create_dataset(test, FLAGS.look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13338, 24, 99)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN parameters\n",
    "N_HIDDEN = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asturkmani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(64, input_shape=(24, 99), dropout=0.2, recurrent_dropout=0.2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 64)                31488     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 99)                6435      \n",
      "=================================================================\n",
      "Total params: 42,083.0\n",
      "Trainable params: 42,083\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Building training model...')\n",
    "model = Sequential()\n",
    "model.add(GRU(N_HIDDEN, dropout_U=0.2, dropout_W=0.2, input_shape=(FLAGS.look_back, FLAGS.inputlength)))\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))\n",
    "model.add(Dense(FLAGS.inputlength, activation='softmax'))  # Add another dense layer with the desired output size.\n",
    "model.compile(loss='mean_squared_error', optimizer = RMSprop(lr=LEARNING_RATE, clipnorm=5))\n",
    "\n",
    "print(model.summary()) # Convenient function to see details about the network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build inference model\n",
    "Note: the inference model will have only one time step as we will feed each predicted character back into the rnn as a seed for predicting the next character. It will also be stateful so as to 'remember' previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13338/13338 [==============================] - 36s - loss: 0.0064    \n",
      "Epoch 2/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0053    \n",
      "Epoch 3/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 4/20\n",
      "13338/13338 [==============================] - 35s - loss: 0.0051    \n",
      "Epoch 5/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 6/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 7/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 8/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 9/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 10/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 11/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 12/20\n",
      "13338/13338 [==============================] - 35s - loss: 0.0051    \n",
      "Epoch 13/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 14/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 15/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 16/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 17/20\n",
      "13338/13338 [==============================] - 34s - loss: 0.0051    \n",
      "Epoch 18/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 19/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n",
      "Epoch 20/20\n",
      "13338/13338 [==============================] - 33s - loss: 0.0051    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0e34d7898>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=trainX, y=trainY, epochs=EPOCHS, batch_size=FLAGS.batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19882072,  0.27684878,  0.16375223,  0.0431559 ,  0.14130197,\n",
       "        0.14666278,  0.12639584,  0.08613823,  0.09059908,  0.10579388,\n",
       "        0.0844413 ,  0.06987463,  0.03658202,  0.08005445,  0.04340203,\n",
       "        0.07851077,  0.06595309,  0.07102945,  0.06807439,  0.079595  ,\n",
       "        0.0531204 ,  0.04994744,  0.04911935,  0.02983522,  0.04010495,\n",
       "        0.04750076,  0.0421062 ,  0.05969562,  0.03622231,  0.04754525,\n",
       "        0.03022813,  0.04791903,  0.04165391,  0.03771969,  0.03483052,\n",
       "        0.04465509,  0.01087982,  0.02505208,  0.04163561,  0.04135814,\n",
       "        0.04643211,  0.01780086,  0.02397728,  0.0343868 ,  0.03971455,\n",
       "        0.00884566,  0.00098255,  0.04289035,  0.04329126,  0.01467267,\n",
       "        0.02386329,  0.03317891,  0.02456031,  0.02826623,  0.00098222,\n",
       "        0.01958819,  0.01607286,  0.02521249,  0.03053943,  0.01932337,\n",
       "        0.02376817,  0.03543101,  0.01542163,  0.02286555,  0.00098236,\n",
       "        0.02057724,  0.03210474,  0.01567693,  0.0270482 ,  0.02432959,\n",
       "        0.00535535,  0.00098242,  0.02702229,  0.02533212,  0.01083584,\n",
       "        0.01783716,  0.01030792,  0.01910297,  0.02802978,  0.01780146,\n",
       "        0.0322674 ,  0.01440228,  0.02468358,  0.02103444,  0.01420377,\n",
       "        0.02061536,  0.01625142,  0.02965894,  0.00186364,  0.01825096,\n",
       "        0.01025258,  0.02206076,  0.02357564,  0.01947164,  0.00098238,\n",
       "        0.02095373,  0.01485581,  0.13342147,  0.37936093])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean( (trainPredict- trainY)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test = np.sqrt(((testY - testPredict) ** 2).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
